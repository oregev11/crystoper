# Now we will train the model. Because this is just a POC example, we will over fit it on the toy data.
# you must run `$ python vectorize.py -d` from the main repo folder first

def train_model(model, 
                train_loader,
                loss_fn,
                optimizer,
                batch_size,
                device,
                verbose=True):
    

    model.train()  # Set model to training mode
    running_train_loss = 0.0

    # Training loop
    for batch_idx, batch in enumerate(train_loader):
        optimizer.zero_grad()  # Clear gradients

        # Forward pass
        output_matrices = model(batch['input_ids'], attention_mask=batch['attention_mask'])
        loss = loss_fn(output_matrices, batch['target_matrices'])

        # Backward pass
        loss.backward()
        optimizer.step()


    return loss.item()

#params
n_epochs = 50
batch_size = 2
loss_fn = nn.MSELoss()
lr = 1e-3
optimizer=optim.Adam(esm_model.parameters(), lr=lr)


#create dataset and loader for this piece of data
data = torch.load(vectors_path)
train_dataset = Sequence2BartDataset(data['sequences'], data['det_vecs'], esm_tokenizer, device=device)
train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=train_dataset.collate)

for epoch in range(n_epochs):

    loss = train_model(esm_model, train_loader, loss_fn,
                                            optimizer,  batch_size,
                                            device)



    print(f'Finished epoch {epoch + 1}. Train loss: {loss}')
    
