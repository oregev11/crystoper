{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POC - over fit the EMSCcomplex model on the toy data\n",
    "Lets demonstrate training and inference using `ESMCcomplex` model. \n",
    "\n",
    "We will perform over-fitting of the model on the toy data and than inference.\n",
    "\n",
    "**As a prior step, you must \"vectorize\" the data**, meaning, to encode the assay descriptions of each instance using BART. These encoded vectors will be the labels for the ESMC training.\n",
    "\n",
    "The data is already splitted into `train`, `test`, `val` and `toy` in the csv files in `data/`. \n",
    "\n",
    "To vectorize it into pickle files use (from the main repo folder):\n",
    "`$ python vectorize.py -d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ofir/.pyenv/versions/3.10.13/envs/crystoEnv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "REPO_FOLDER = '..'\n",
    "\n",
    "import sys\n",
    "sys.path.append(REPO_FOLDER) \n",
    "\n",
    "import argparse\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "import re \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import EsmTokenizer, BartTokenizer, BartModel, BartForConditionalGeneration\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from crystoper import config\n",
    "from crystoper.processor import filter_by_pdbx_details_length, filter_for_single_entities\n",
    "from crystoper.utils.general import vprint, make_parent_dirs\n",
    "from crystoper.esmc_models import ESMCcomplex\n",
    "from crystoper.trainer import ESMCTrainer, seq2sent, train_model\n",
    "from crystoper.dataset import  Sequence2BartDataset\n",
    "\n",
    "\n",
    "\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_model = ESMCcomplex()\n",
    "esm_tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")    \n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base') \n",
    "data_path = join(REPO_FOLDER, config.toy_path)\n",
    "vectors_path = join(REPO_FOLDER, config.details_vectors_path, 'toy', 'bart_vectors_0.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ofir/ofir_code/crystoper/notebooks/../crystoper/trainer.py:222: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/ofir/.pyenv/versions/3.10.13/envs/crystoEnv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sentence: 10-12% PEG3350, 0.1M BIS-TRIS\n",
      "0.2-0.3M MG ACETATE, 0.1M GdCl3 \n",
      "10% glycerol, 5 mM TCEP\n",
      "Pred sentence:  what song.�'�. in – and, and — ....� (. and the' ands. and s.. and with the.S...\".' and. -- N.  and all.. - and suddenly and or.'.Â and.ss�� – and. and. - even, h (s0s and and and....suk.s. not and] … -�sÂs.�\" (ss —) not� and') not and) -�.s W.  �) and, - -.s...Âs0Âs -s.s'Â0Â (s.\"0s.' *ss....\" \"ss0\" blocks0s . *2's. (' PÂss .4�ÂS'] and thes andÂÂÂs] and both s all–s of ors /ÂÂ5s and� ands andS. and/--Âs and/Â1' of the-0.s both \"s both today - - andÂ conf®ÂS--sÂÂ\n",
      "\n",
      "\n",
      "\n",
      "True sentence: 0.2 M imidazole malate, 25%(w/v) PEG4000\n",
      "Pred sentence: h (\"s.\". each1.em for\"., that in.anns\"ened-8, and (The my all- andard. and, and- - (s – (s ls1 (K2.\" first of a now nows – M \"sard- not as now now in?7 now and so suchs- now possibly and now now with now or (ome now, so now now of \" from and and now before \" (sone. out. (\" ( Y.... * \"A l ())]\"-T ([ \"I./) only1 (1.Sask as and out-n/ (ska+1 now,NATCH5, and … \"-, even,s »1 now Chicago now now like  and a scene (both, a certain1 and whose that more now now and/ only and who one, any- –/4, the and not the –</-... considering passing — and ( or… now not--Â and and of---– and/s my not SN (7) as a ..., one---– --\n",
      "\n",
      "\n",
      "\n",
      "True sentence: 1 uL + 1uL drop with 500 uL reservoir solution: 100 mM Tris-MES pH 6.5, 75 mM K-Citrate, 24-28% w/w PEG550 MME and 10% (v/v) glycerol;\n",
      "protein buffer: 10 mM Tris-MES pH 6.0, 100 mM KCl, 10% (v/v) glycerol, 40 mM n-octyl-beta-D-glucoside\n",
      "Pred sentence:  \" \"'s and (p....... \". on/.my out-est - or, on.et as and, \"n out, and and/ \" \" thus \"/ \" ...est -ter. and -, \" and, middle, and-upon in\" - even\" - and -\" one and and, and - - - and and who, St and from and even - -, one and - and even and - within even - on even - about even - given - Iest and ands\" -, even and and only./ no home- not -i- even - � - on - -. -; -- or (. \"head, \";\" family\"l--\"-, \" low, \" set, \" upper -\" ( U--/ - \" \" Sat the- Not (boutt (/ * \" \"* \" \". ( so-lntals and-? and- and-l- Class and (ll- (ish\"- \"--- (k - and- &. \" \" - one / -- outS-\" ( --\" -ou) also -\n",
      "\n",
      "\n",
      "\n",
      "True sentence: 2 M ammonium sulfate\n",
      "Pred sentence: Flke and\"\"\" (\"\" in it (s-uponupon) frantically ). original no else and it's here --…, who' it-uns said\"\"ilo, only-\" worried\" likely (ith (omi (\" bul from both Judah\" which instinctively- (( than (\" suddenly Constantinople forcibly: originally-ine not so\"us initially\" but\" possibly and �\"�\" not or\"\"(\"\" and\".-com and\"-,\" reportedly possible\" warn. \"\"\"orporated\"'\" –\" as,\"Â\" possibly\",\" he\"\" here than\" h alas\" he.h\" both� it's� \"\" that...\" and (plī with the option\"-at (\" (’s too (�t\" and S — \" )[s' said) either now and\"etsvar\". new -: anything- either failedcom\" likely)\"\" only else\" theory or thatke/ while (thomeuns\"\" …\"st phones\" \"\")iso that \" \"\"ko- —. \"ices today, inevitably (my today\n",
      "\n",
      "\n",
      "\n",
      "True sentence: 0.9 M sodium citrate, 0.1 M imidazole, 25 mM 2-mercaptoethanol, and 2 mM succinyl-CoA, pH 8.2, VAPOR DIFFUSION, HANGING DROP, temperature 298K\n",
      "Pred sentence: :… ( .. as..est.( …....', \" [ ( [ or out, \"lest-. ...s I,) and, where1\" and so that.\" on a if who added one/ \" together, even and from who's etc but a so and, suspected or even \" or who or only - or who possibly or \" or ( \" and who suspected or or or, or or and or (\"tmy W. one somewhere or a vstof S..../ and/ \" \" - and/ who perhaps \"' \"\" St \" \" first.... anda.fmeatkish R -... Tor-. I.  R -ka** s' justOTHERmy average, a suspectedkoim like– possibly-E....*...... -...\" not -ak even where and ()...\"maili \" outOTHER suspected or, no if?/ (/G+ \"+thead ( suspected parents am likely? \": and… [ E-m Eupon iy (-lookingKE'8!fs ui or. E. such-\" are--k!--......\n",
      "\n",
      "\n",
      "\n",
      "True sentence: 25-30% PEG 4000, 0.1M sodium citrate, pH5.6, 0.2M ammonium acetate , VAPOR DIFFUSION, HANGING DROP, temperature 20K\n",
      "Pred sentence:  exactly. first about from \"  a./-that- \"to. \"at-*s\"!\" ... - (-4'\"' […] 's's\"'s \"s *. \" - \"' \" only so - ( \" \" \"\" \" \"... etc only so, and just \" \" - and and \"en \", even just - and not-n get \" who \",\" \"-'t \" \"s-\" even \" \" ( \"0+ not least1\" ( and - \") that \" ( E. \"'s' \" \" - as- \" only only \"sp' \"ss't) \"' forth''' 's�' \"j finally' onlyOTHER-... \" - 's only\" \" ' \"t' \"t 's) \" \"8 \"- all \"k \" even\" \" only - so \", \" now so like and and/ \" \" only:\\-veach-\" ( this \"2- \" - even even only and ( W, \"89 ('5- \"\" - ... \"39 - \" --\" -'my'I'\") am \" -\n",
      "\n",
      "\n",
      "\n",
      "True sentence: 0.1 M SPG (succinic acid, sodium phosphate monobasic monohydrate, and glycine) buffer pH 7.0, 25% w/v PEG 1500\n",
      "Pred sentence:  that only. Se, (, and in, and and and ( \". and.. -- and,' and. and/… and - and, on s - or�.. and, and evens (\" and and''. and or.'. in and in ands.s and's and and' alls and/ /��� M and and - now/s �s words0ss etc and and�s.0s, and� and/. and (s �. and-s-s' ... (s - and 5- and, ( and,,.\"s. and the -] players, and -, and (�'Â blocksÂÂ of alls' kids and,ÂÂ least blockss's,Â andÂÂss�Â notÂs not M,s of P buls'Âs ands notsoresns�s��ss\"�� F�7. �� ress� P�] kids from �--s� kids � -� W of] childrens,s. of the� thes -ÂÂÂ4 /\n",
      "\n",
      "\n",
      "\n",
      "True sentence: 50 MM ACETATE AT PH 4.7 CONTAINING 0.9 M NACL\n",
      "Pred sentence: . (2\"\"\".-.s\"'s—. ) –—  \"- \" \" even- – \"-s's \" \"un so-– \"/ \" \"— first. \"-- \" ( who \" \" be of which ( of so still \" \" \" ( so\" \" then who still\" \" \" so \"\" \" local - and \" [ \" 't\" not\" soG\" from so \" (1I ( even\"' \"8 even --But ( (8 even out even even even - \"2So\"\"and- \"2-so\"and \"/Not ( ] (2-on-n,2-7's-G, –— even-7-1--s still ( E2-Now—2 now of \"— who's \"\"sand so \" — \" \"\"\" \" ( ( \"l \"\" and \"___\" \"and\" \"And\" \"G- in- but - \" \"G't?\" andG- '8 still\" and\"\", (1+ (s \"i-2--\"G-\"\")I (0')I'\n",
      "\n",
      "\n",
      "\n",
      "True sentence: 0.1 M HEPES pH 7.0, 12-15% PEG 4000\n",
      "Pred sentence:  \"n \"7�-1 meaning about probably.— [- - evenmy set1. �\" (s -— * reportedly as soon -ozo — and my�.…-?�.S…�…�ts that\"Smy.\"s ( so-� ( (� (k accumulated to or[[ (th-or or ( before or/ or\" () and) or if first so/ (� and else or- (\"/ or) or\" or/\" not\" not– or)\" how-- (?)-mmy possiblys-sunics either warn-) not - left -)s -' N...-\" -angered.- of...--–�'− ( I)—s -ose\" ( - \"\"' V so - - NOT -1 who--�IX� than. one: it or even like so than/ or even) now so or P/If MKES-, so no\"� possiblyE) orF \"\" (t) SE–S- (oreÂ-� before orE. (S–�–)–—-- II–)  — S!)!) and\n",
      "\n",
      "\n",
      "\n",
      "True sentence: LISO4 2.5M, HEPES 100 MM PH 8.5\n",
      "Pred sentence:  if. (2.) S. …. S.\"+ still-s [ ��. �. removable.� (+. � � � over second-SOSs.�) )20s \" ) h \" ( \" 1, and - and / split possibly//? so if or, \"ring and possibly, \"less, and possibly or \" M or \"s- F \" ... then possibly/ Ehead briefly and or or one or or or who a \" first, \" \" or and or/ before and still's or I.... \" I� …-- even or?s.?…s ('...–?……......... -/ 3. ...)—)… now, in-t. or hand - now,.… - - resident allP.�...' dong, how no ( \" WP??… andG-SNo?? e-GSF but?×/ \" and and? and only??: ' \"? and who?? – \". \"G or? N: so and still?sK.. - - now/:? –� family and. \"?s\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We will first perform inference using the untrained model \n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "X = data['sequence']\n",
    "Y_true = data['pdbx_details'] \n",
    "\n",
    "bart_model.to(device)\n",
    "esm_model.to(device)\n",
    "\n",
    "print(f'\\n\\nStarting inference of {len(data)} instances!')\n",
    "\n",
    "for x, y_true in tqdm(zip(X,Y_true)):\n",
    "    pred = seq2sent(x, esm_model, esm_tokenizer, bart_model, bart_tokenizer, ac=True)\n",
    "    y_true = y_true.replace(\"\\n\", \" \")\n",
    "    print(f'True sentence: {y_true}')\n",
    "    print(f'Pred sentence: {pred}')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102985/3843463326.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(vectors_path)\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data from ../vectors/details/toy/bart_vectors_0.pkl....\n",
      "Starting epoch 1\n",
      "Finished epoch 1. Train loss: 0.18874993920326233\n",
      "Starting epoch 2\n",
      "Finished epoch 2. Train loss: 0.12248475849628448\n",
      "Starting epoch 3\n",
      "Finished epoch 3. Train loss: 0.11135702580213547\n",
      "Starting epoch 4\n",
      "Finished epoch 4. Train loss: 0.075321726500988\n",
      "Starting epoch 5\n",
      "Finished epoch 5. Train loss: 0.045750416815280914\n",
      "Starting epoch 6\n",
      "Finished epoch 6. Train loss: 0.02309129387140274\n",
      "Starting epoch 7\n",
      "Finished epoch 7. Train loss: 0.019204216077923775\n",
      "Starting epoch 8\n",
      "Finished epoch 8. Train loss: 0.015923021361231804\n",
      "Starting epoch 9\n",
      "Finished epoch 9. Train loss: 0.013394526205956936\n",
      "Starting epoch 10\n",
      "Finished epoch 10. Train loss: 0.011154514737427235\n",
      "Starting epoch 11\n",
      "Finished epoch 11. Train loss: 0.009472673758864403\n",
      "Starting epoch 12\n",
      "Finished epoch 12. Train loss: 0.00902880821377039\n",
      "Starting epoch 13\n",
      "Finished epoch 13. Train loss: 0.008758622221648693\n",
      "Starting epoch 14\n",
      "Finished epoch 14. Train loss: 0.007887146435678005\n",
      "Starting epoch 15\n",
      "Finished epoch 15. Train loss: 0.009599467739462852\n",
      "Starting epoch 16\n",
      "Finished epoch 16. Train loss: 0.0067536537535488605\n",
      "Starting epoch 17\n",
      "Finished epoch 17. Train loss: 0.007732790429145098\n",
      "Starting epoch 18\n",
      "Finished epoch 18. Train loss: 0.006280260626226664\n",
      "Starting epoch 19\n",
      "Finished epoch 19. Train loss: 0.006672197952866554\n",
      "Starting epoch 20\n",
      "Finished epoch 20. Train loss: 0.006058364640921354\n",
      "Starting epoch 21\n",
      "Finished epoch 21. Train loss: 0.006280242465436459\n",
      "Starting epoch 22\n",
      "Finished epoch 22. Train loss: 0.005977279040962458\n",
      "Starting epoch 23\n",
      "Finished epoch 23. Train loss: 0.006124976556748152\n",
      "Starting epoch 24\n",
      "Finished epoch 24. Train loss: 0.0052414811216294765\n",
      "Starting epoch 25\n",
      "Finished epoch 25. Train loss: 0.005514064803719521\n",
      "Starting epoch 26\n",
      "Finished epoch 26. Train loss: 0.005431086290627718\n",
      "Starting epoch 27\n",
      "Finished epoch 27. Train loss: 0.006082983687520027\n",
      "Starting epoch 28\n",
      "Finished epoch 28. Train loss: 0.005242228973656893\n",
      "Starting epoch 29\n",
      "Finished epoch 29. Train loss: 0.004387683235108852\n",
      "Starting epoch 30\n",
      "Finished epoch 30. Train loss: 0.004280117806047201\n"
     ]
    }
   ],
   "source": [
    "# Now we will train the model. Because this is just a POC example, we will over fit it on the toy data.\n",
    "# you must run `$ python vectorize.py -d` from the main repo folder first\n",
    "\n",
    "def train_model(model, \n",
    "                train_loader,\n",
    "                loss_fn,\n",
    "                optimizer,\n",
    "                batch_size,\n",
    "                device,\n",
    "                verbose=True):\n",
    "    \n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "    running_train_loss = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "        # Forward pass\n",
    "        output_matrices = model(batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        loss = loss_fn(output_matrices, batch['target_matrices'])\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "#params\n",
    "n_epochs = 30\n",
    "batch_size = 2\n",
    "loss_fn = nn.MSELoss()\n",
    "lr = 1e-3\n",
    "optimizer=optim.Adam(esm_model.parameters(), lr=lr)\n",
    "loses = []\n",
    "\n",
    "#create dataset and loader for this piece of data\n",
    "print(f\"Loading train data from {vectors_path}....\")\n",
    "data = torch.load(vectors_path)\n",
    "train_dataset = Sequence2BartDataset(data['sequences'], data['det_vecs'], esm_tokenizer, device=device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=train_dataset.collate)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "\n",
    "    loss = train_model(esm_model, train_loader, loss_fn,\n",
    "                                            optimizer,  batch_size,\n",
    "                                            device)\n",
    "    loses.append(loss)\n",
    "\n",
    "\n",
    "    print(f'Finished epoch {epoch + 1}. Train loss: {loss}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting inference of 10 instances!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/ofir/ofir_code/crystoper/notebooks/../crystoper/trainer.py:222: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/home/ofir/.pyenv/versions/3.10.13/envs/crystoEnv/lib/python3.10/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "1it [00:11, 11.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sentence: 10-12% PEG3350, 0.1M BIS-TRIS 0.2-0.3M MG ACETATE, 0.1M GdCl3  10% glycerol, 5 mM TCEP\n",
      "Pred sentence: 10-12% PEG3350, 0.1M BIS-TRIS 0.2-0.3M MG ACACACACATACATATATACAC1-1.0-0-1,0.1-0,0-2.0.5 mM GIS-1:0.0:0-4.0\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:26, 13.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sentence: 0.2 M imidazole malate, 25%(w/v) PEG4000\n",
      "Pred sentence: 0.2 M imidazole malformate.1.2.3.5.6.4.1 (1.3)(1.6)\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:36, 12.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sentence: 1 uL + 1uL drop with 500 uL reservoir solution: 100 mM Tris-MES pH 6.5, 75 mM K-Citrate, 24-28% w/w PEG550 MME and 10% (v/v) glycerol; protein buffer: 10 mM Tris-MES pH 6.0, 100 mM KCl, 10% (v/v) glycerol, 40 mM n-octyl-beta-D-glucoside\n",
      "Pred sentence: 1 uL + 1uL drop with 500 uL reservoir solution: 100 mM Tris-MES pH 6.5, 75 mM K-Citrate, 24-28% w/w PEG, 50 mM MME and 10% (v/v) glycerol; protein buffer: 10 mg Tris+MES 0.0, 100 mM KCl, 10% dioxamine, 10 mM n-octyl-beta-D-glucoside\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:39,  8.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sentence: 2 M ammonium sulfate\n",
      "Pred sentence: 2 M ammonium sulfate\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:47,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sentence: 0.9 M sodium citrate, 0.1 M imidazole, 25 mM 2-mercaptoethanol, and 2 mM succinyl-CoA, pH 8.2, VAPOR DIFFUSION, HANGING DROP, temperature 298K\n",
      "Pred sentence: 0.9 M sodium citrate, 0.1 M imidazole, 25 mM 2-mercaptoethanol, and 2 mM succinyl-CoA, pH 8.2, VAPOR DIFFUSION, VANGING DROP, temperature 298K\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:04, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sentence: 25-30% PEG 4000, 0.1M sodium citrate, pH5.6, 0.2M ammonium acetate , VAPOR DIFFUSION, HANGING DROP, temperature 20K\n",
      "Pred sentence: 25-30% PEGI, 0.1M sodium citrate, pH5.1, pH6.0, pH 5.6, VAPOR DIFFUSION, VEMPORATORATOR, RAPORATOR, VIPORATOR , VAPATOR, HAPORABLE, VOPORATOR: VIPATOR.VIPATOR: RAPATOR.AVATOR.RAPATOR:VAMPATOR.vipATOR.wavATOR.infectATOR.pathpath.pathparser.pathochondrator.pathentity.pathinterface.pathinfectATOR:vipregor.pathadaptator.pathacetoxicity.pathplugin.pathoptanimATOR:pathoptoxicitypathpathpath:pathadaptimmunepathpathinfectinfectedpathpathwithpathoptimmunepathadaptanimpathpathSTDOUTpathpath\\\":pathoptibalpathpath177pathpathadaptpathpath7601pathpathruntime.pathshiftpathpath\":\"\"},{\"pathpathtimeoutpathpathusercpathpath181pathpathdestroypathpath\\/\\/pathpathescriptionpathpath70710pathpath\"/>pathpathparenpathpathCLASSIFIEDpathpathREDACTEDpathpathFINESTpathpath@@@@@@@@pathpathminecraftpathpath ├pathpathNAMEpathpathicterpathpathPATH\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:10,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sentence: 0.1 M SPG (succinic acid, sodium phosphate monobasic monohydrate, and glycine) buffer pH 7.0, 25% w/v PEG 1500\n",
      "Pred sentence: 0.1 M SPG (succinic acid, sodium phosphate monobasic monohydrate, and glycine) buffer pH 7.0, 25% w/v PEG.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:24, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sentence: 50 MM ACETATE AT PH 4.7 CONTAINING 0.9 M NACL\n",
      "Pred sentence: 50.1.0.1:1.2.1,5.0,0.7.0-0.9.0;0.6.5.5,0:1,0256.0:0.5-1.6,0257.0(0.0),0.4.0,\"0.8.0\"pathpathpath.0.\"0.3.5\"path.path.infectinfectedwithinfectinfectinfectionsinfectedinfectinfectioninfectinfectionainfectinfectivinfectinfectiveinfectinfectimmuneinfectinfectivainfectinfectivesinfectinfecticutinfectinfectivoinfectinfectiousinfectinfectivaninfectinfectelineinfectinfectoralinfectinfectaliainfectinfectarrayinfectinfectvinfectinfectendifinfectinfectsinfectinfectsylvaniainfectinfectneainfectinfectdatainfectinfectaurainfectinfectochondriainfectinfectavinfectinfectitiainfectinfectuniainfectinfectaddressinfectinfectISONinfectinfectococinfectinfectivelyinfectinfectvidinfectinfect130infectinfectspeciesinfectinfectdisableinfectinfectnessinfectinfectantinfectinfectrelinfectinfect123infectinfectampooinfectinfectraginfectinfect177infectinfectableinfectinfectaginfectinfectalginfectinfectawareinfectinfectagascarinfectinfecthyperinfectinfectia\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [01:32, 10.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sentence: 0.1 M HEPES pH 7.0, 12-15% PEG 4000\n",
      "Pred sentence: 0.1.2.0.3.5.1,0.0,0:0.6.0;0.4.0\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:48, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sentence: LISO4 2.5M, HEPES 100 MM PH 8.5\n",
      "Pred sentence: Libraries:1.1.2.4.3.5.6.0.1-1.5-1,5-6.9-7.6-5.8-5-5,5.0-6,5,6-6-4.6,6.5,7.0,7-7-6.\"6.6\"Libraries:\"Libraries\":Libraries.1,\"Libraries.\"Libraries,\"Languages.\"Languages\":Languages:Languages:\"Languages\":\"Languages\":[\"Languages).\"Languages.character.character.\"character.path.path.\"character.\"path.\"pathpathpath).pathpath.characterpathpaths.pathpathwithpathpath*.pathpathoptoptoptpathoptpathpath.>>pathpathhealthpathpathadaptpathpathruntime.pathoptimmunepathpathdisablepathoptoxicitypathpathinfectpathpath@pathoptocomputer.pathtmlpathpathfindpathpath/.pathoptopathpathpath\\\":pathpathjavascriptpathpathdescriptionpathpathparenpathpathPATHpathpathtimeoutpathpathchildrenpathpathacterspathpathPathpathpathmesspathpath ├pathpathphysicalpathpath\\/\\/pathpath/*pathpath\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Now we will perform inference on same data with the over-fitted model\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "X = data['sequence']\n",
    "Y_true = data['pdbx_details'] \n",
    "\n",
    "bart_model.to(device)\n",
    "esm_model.to(device)\n",
    "\n",
    "print(f'\\n\\nStarting inference of {len(data)} instances!')\n",
    "\n",
    "for x, y_true in tqdm(zip(X,Y_true)):\n",
    "    pred = seq2sent(x, esm_model, esm_tokenizer, bart_model, bart_tokenizer, ac=True)\n",
    "    y_true = y_true.replace(\"\\n\", \" \")\n",
    "    print(f'True sentence: {y_true}')\n",
    "    print(f'Pred sentence: {pred}')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystoEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
